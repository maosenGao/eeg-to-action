{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "dUIZ65Tu0k-7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mLcUM3Tn0lUQ"
   },
   "source": [
    "# Install a kaggle-cli to download the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 1014
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 11310,
     "status": "ok",
     "timestamp": 1525529067081,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "AqQXs6I321Cv",
    "outputId": "0ec9cd32-a587-44c0-f5a8-9d05001d6209"
   },
   "outputs": [],
   "source": [
    "!pip install kaggle-cli"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Tyytf5OH0rtp"
   },
   "source": [
    "# Download the data (insert your username and password)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 88
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 32880,
     "status": "ok",
     "timestamp": 1525529112335,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "61LXLiY82-fA",
    "outputId": "1f5b2157-1680-42ea-f529-6f512a4c1e75"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "downloading https://www.kaggle.com/c/grasp-and-lift-eeg-detection/download/train.zip\r\n",
      "\n",
      "train.zip 100% |####################################| Time: 0:00:27  33.0 MiB/s\n",
      "\n"
     ]
    }
   ],
   "source": [
    "!kg download -u INSERT-USERNAME -p INSERT-PASSWORD -c grasp-and-lift-eeg-detection -f train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5cX3CXhZ0wqS"
   },
   "source": [
    "# Check the data and unzip it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1684,
     "status": "ok",
     "timestamp": 1525529115126,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "rms8g6gQ5JxW",
    "outputId": "a46ba305-ea27-4ec4-efee-cd720a01ac4b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "datalab  train.zip\r\n"
     ]
    }
   ],
   "source": [
    "!ls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 3465
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 34033,
     "status": "ok",
     "timestamp": 1525529154390,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "rlnYcxb-4Oul",
    "outputId": "3660a050-e067-496c-d8f8-00c0dfe780e8"
   },
   "outputs": [],
   "source": [
    "!unzip train.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "IIO8dioj01t1"
   },
   "source": [
    "# Learning phase.\n",
    "Load several series of different subjects (a sample of the all dataset). With time_steps and subsample you decide how many temporal steps to take (e.g., 1000 sequential data points) and how to subsample them (e.g. take 1 every 100). We then define a convolutional neural network that computes features doing \"temporal convolutions\", i.e. it uses filters over both the different EEG measurments at a particular moment and future/past measurments. \\\\\n",
    "It uses a generator to load the data, that otherwise could be to heavy to store in memory (since it is multiplied by the time_steps value). Notice that the data points are all taken before the y value to predict to ensure a causal relationship. \\\\\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 2091
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 30615,
     "status": "error",
     "timestamp": 1525539903799,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "AFoTOqC-4Rjl",
    "outputId": "d5f1e05a-787a-4c09-b1b4-a4e3056bd501"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "load = 0 #use 0 if the data is already loaded\n",
    "\n",
    "time_steps = 1000\n",
    "\n",
    "subsample = 100\n",
    "\n",
    "x_paths = [\"train/subj1_series1_data.csv\", \"train/subj1_series2_data.csv\", \"train/subj1_series3_data.csv\", \"train/subj1_series4_data.csv\", \\\n",
    "          \"train/subj2_series1_data.csv\", \"train/subj2_series2_data.csv\", \"train/subj2_series3_data.csv\", \"train/subj2_series4_data.csv\", \\\n",
    "          \"train/subj3_series1_data.csv\", \"train/subj3_series2_data.csv\", \"train/subj3_series3_data.csv\", \"train/subj3_series4_data.csv\", \\\n",
    "          \"train/subj4_series1_data.csv\", \"train/subj4_series2_data.csv\", \"train/subj4_series3_data.csv\", \"train/subj4_series4_data.csv\",\\\n",
    "          \"train/subj5_series1_data.csv\", \"train/subj5_series2_data.csv\", \"train/subj5_series3_data.csv\", \"train/subj5_series4_data.csv\",\\\n",
    "          \"train/subj6_series1_data.csv\", \"train/subj6_series2_data.csv\", \"train/subj6_series3_data.csv\", \"train/subj6_series4_data.csv\"]\n",
    "y_paths = [\"train/subj1_series1_events.csv\", \"train/subj1_series2_events.csv\", \"train/subj1_series3_events.csv\", \"train/subj1_series4_events.csv\", \\\n",
    "          \"train/subj2_series1_events.csv\", \"train/subj2_series2_events.csv\", \"train/subj2_series3_events.csv\", \"train/subj2_series4_events.csv\", \\\n",
    "          \"train/subj3_series1_events.csv\", \"train/subj3_series2_events.csv\", \"train/subj3_series3_events.csv\", \"train/subj3_series4_events.csv\", \\\n",
    "          \"train/subj4_series1_events.csv\", \"train/subj4_series2_events.csv\", \"train/subj4_series3_events.csv\", \"train/subj4_series4_events.csv\", \\\n",
    "          \"train/subj5_series1_events.csv\", \"train/subj5_series2_events.csv\", \"train/subj5_series3_events.csv\", \"train/subj5_series4_events.csv\", \\\n",
    "          \"train/subj6_series1_events.csv\", \"train/subj6_series2_events.csv\", \"train/subj6_series3_events.csv\", \"train/subj6_series4_events.csv\"]\n",
    "\n",
    "\n",
    "if load:\n",
    "  x_data = []\n",
    "  for x_path in x_paths:\n",
    "    with open(x_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] == \"i\": continue\n",
    "            line_array = []\n",
    "            for word in range(len(line.split(','))-1):\n",
    "                word+=1\n",
    "                line_array.append(int(line.split(',')[word]))\n",
    "            line_array = np.asarray(line_array)\n",
    "            x_data.append(line_array)\n",
    "  x_data = np.asarray(x_data)    \n",
    "        \n",
    "  \n",
    "  y_data = []\n",
    "  for y_path in y_paths:\n",
    "    with open(y_path) as file:\n",
    "        for line in file:\n",
    "            if line[0] == \"i\": continue\n",
    "            line_array = []\n",
    "            for word in range(len(line.split(','))-1):\n",
    "                word+=1\n",
    "                line_array.append(int(line.split(',')[word]))\n",
    "            line_array = np.asarray(line_array)\n",
    "            y_data.append(line_array)\n",
    "  y_data = np.asarray(y_data)    \n",
    "        \n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout\n",
    "from keras.layers import Embedding\n",
    "from keras.layers import LSTM, CuDNNLSTM, BatchNormalization, Conv2D, Flatten, MaxPooling2D, Dropout\n",
    "from keras.optimizers import Adam\n",
    "\n",
    "model = Sequential()\n",
    "#model.add(CuDNNLSTM(128, input_shape = (time_steps//subsample, 32)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (7,7), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 32, 1)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (5,5), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 32, 1)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
    "model.add(Conv2D(filters = 64, kernel_size = (3,3), padding = \"same\", activation = \"relu\", input_shape = (time_steps//subsample, 32, 1)))\n",
    "model.add(BatchNormalization())\n",
    "#model.add(MaxPooling2D(pool_size = (3,3)))\n",
    "model.add(Flatten())\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(32, activation = \"relu\"))\n",
    "model.add(BatchNormalization())\n",
    "# model.add(Dropout(0.2))\n",
    "model.add(Dense(6, activation = \"sigmoid\"))\n",
    "\n",
    "\n",
    "adam = Adam(lr = 0.001)\n",
    "\n",
    "model.compile(optimizer = adam, loss = \"categorical_crossentropy\", metrics = [\"accuracy\"])\n",
    "\n",
    "model.summary()\n",
    "\n",
    "def generator(batch_size):\n",
    "    while 1:\n",
    "        \n",
    "        x_time_data = np.zeros((batch_size, time_steps//subsample, 32))\n",
    "        yy = []\n",
    "        for i in range(batch_size):\n",
    "            random_index = np.random.randint(0, len(x_data)-time_steps)\n",
    "            x_time_data[i] = x_data[random_index:random_index+time_steps:subsample]\n",
    "            yy.append(y_data[random_index + time_steps])\n",
    "        yy = np.asarray(yy)\n",
    "        yield x_time_data.reshape((x_time_data.shape[0],x_time_data.shape[1], x_time_data.shape[2], 1)), yy\n",
    "        \n",
    "        \n",
    "    \n",
    "model.fit_generator(generator(32), steps_per_epoch = 5000, epochs = 3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "eDajD6dD3Skv"
   },
   "source": [
    "# Test phase.\n",
    "Here we load some unseen data. We use, as a metric, the roc_auc_score, as defined in the Kaggle challenge. We create a new generator, and sample this metric on num_test new data points, averaging the score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "sha49lFS5NmQ"
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_curve, auc, roc_auc_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "3jtFc22tyklL"
   },
   "outputs": [],
   "source": [
    "x_val_path = \"train/subj4_series7_data.csv\"\n",
    "y_val_path = \"train/subj4_series7_events.csv\"\n",
    "x_val_data = []\n",
    "with open(x_val_path) as file:\n",
    "    for line in file:\n",
    "        if line[0] == \"i\": continue\n",
    "        line_array = []\n",
    "        for word in range(len(line.split(','))-1):\n",
    "            word+=1\n",
    "            line_array.append(int(line.split(',')[word]))\n",
    "        line_array = np.asarray(line_array)\n",
    "        x_val_data.append(line_array)\n",
    "x_val_data = np.asarray(x_val_data)    \n",
    "        \n",
    "  \n",
    "y_val_data = []\n",
    "with open(y_val_path) as file:\n",
    "    for line in file:\n",
    "        if line[0] == \"i\": continue\n",
    "        line_array = []\n",
    "        for word in range(len(line.split(','))-1):\n",
    "            word+=1\n",
    "            line_array.append(int(line.split(',')[word]))\n",
    "        line_array = np.asarray(line_array)\n",
    "        y_val_data.append(line_array)\n",
    "y_val_data = np.asarray(y_val_data)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     }
    },
    "colab_type": "code",
    "id": "fEVzINF4y8XJ"
   },
   "outputs": [],
   "source": [
    "def val_generator():\n",
    "    while 1:\n",
    "        batch_size = 1\n",
    "        x_time_data = np.zeros((batch_size, time_steps//subsample, 32))\n",
    "        yy = []\n",
    "        for i in range(batch_size):\n",
    "            random_index = np.random.randint(0, len(x_val_data)-time_steps)\n",
    "            x_time_data[i] = x_val_data[random_index:random_index+time_steps:subsample]\n",
    "            yy.append(y_val_data[random_index + time_steps])\n",
    "        yy = np.asarray(yy)\n",
    "        yield x_time_data.reshape((x_time_data.shape[0],x_time_data.shape[1], x_time_data.shape[2], 1)), yy\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5406,
     "status": "ok",
     "timestamp": 1525539252101,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "laZHiRUeuYQ6",
    "outputId": "bf69b125-4231-4b46-c34b-c5d247efd271"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean  0.8485916666666667\n"
     ]
    }
   ],
   "source": [
    "gen_data = val_generator()\n",
    "scores = []\n",
    "num_test = 1000\n",
    "for i in range(num_test):\n",
    "  x_test, y_test = next(gen_data)\n",
    "  while not 1 in y_test:\n",
    "    #print(y_test)\n",
    "    x_test, y_test = next(gen_data)\n",
    "\n",
    " # print(y_test)\n",
    "  y_out = model.predict(x_test).reshape((6,1))\n",
    " # print(y_out)\n",
    "  scores.append(roc_auc_score(y_test.reshape((6,1)), y_out))\n",
    "scores = np.asarray(scores)\n",
    "print(\"Mean \", np.mean(scores))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "colab": {
     "autoexec": {
      "startup": false,
      "wait_interval": 0
     },
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 1433,
     "status": "ok",
     "timestamp": 1525531786038,
     "user": {
      "displayName": "Norman Di Palo",
      "photoUrl": "//lh5.googleusercontent.com/-hwpkiYzSctg/AAAAAAAAAAI/AAAAAAAAAkg/I59DXttMdKw/s50-c-k-no/photo.jpg",
      "userId": "109405231849327541723"
     },
     "user_tz": -120
    },
    "id": "0-eKXupZvhID",
    "outputId": "34d51edd-11e7-4f55-ad03-0c132d40bf2c"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1)"
      ]
     },
     "execution_count": 53,
     "metadata": {
      "tags": []
     },
     "output_type": "execute_result"
    }
   ],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "default_view": {},
   "name": "eeg.ipynb",
   "provenance": [],
   "version": "0.3.2",
   "views": {}
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
